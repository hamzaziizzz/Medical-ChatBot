{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc7d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfee5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\".\\\\data\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4440121",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "We need to find if two questions are similar. In face recognition, we use siamese networks to solve similar problem but for faces. So, we'll try using siamese networks here - only we'll use LSTMs instead of CNNs since LSTMs are suited for sequences.\n",
    "\n",
    "0. Check the data. If we have enough q-ids for which we have duplicates available, then we could train the whole thing via triplet loss. If so, follow plan a, else plan b.\n",
    "\n",
    "1. Pre-processing\n",
    "    - Remove questionmarks throughout\n",
    "    - Remove stop-words (Save one which keeps stop-words as well)\n",
    "\n",
    "2. Convert to vectors\n",
    "3. Divide in 70/30 split. (Also try 80/20 split)\n",
    "4. Pass through siamese LSTMs\n",
    "\n",
    "**Plan a**\n",
    "5. Use squared distance\n",
    "\n",
    "**Plan b**\n",
    "5. Use triplet loss (find all those q-ids which have duplicates available for them first. See if it makes sense to use triplet loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aec5b0",
   "metadata": {},
   "source": [
    "# Analysis of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0109f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59716c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                  id           qid1           qid2   is_duplicate\ncount  363861.000000  363861.000000  363861.000000  363861.000000\nmean   181930.000000  201899.281913  204884.863951       0.371502\nstd    105037.767486  144924.825062  146663.968132       0.483207\nmin         0.000000       1.000000       2.000000       0.000000\n25%     90965.000000   70779.000000   70942.000000       0.000000\n50%    181930.000000  179999.000000  184182.000000       0.000000\n75%    272895.000000  321295.000000  327744.000000       1.000000\nmax    363860.000000  493887.000000  493889.000000       1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>363861.000000</td>\n      <td>363861.000000</td>\n      <td>363861.000000</td>\n      <td>363861.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>181930.000000</td>\n      <td>201899.281913</td>\n      <td>204884.863951</td>\n      <td>0.371502</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>105037.767486</td>\n      <td>144924.825062</td>\n      <td>146663.968132</td>\n      <td>0.483207</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>90965.000000</td>\n      <td>70779.000000</td>\n      <td>70942.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>181930.000000</td>\n      <td>179999.000000</td>\n      <td>184182.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>272895.000000</td>\n      <td>321295.000000</td>\n      <td>327744.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>363860.000000</td>\n      <td>493887.000000</td>\n      <td>493889.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c17dcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                  id           qid1           qid2  is_duplicate\ncount  135175.000000  135175.000000  135175.000000      135175.0\nmean   181735.176741  156901.917507  157247.986292           1.0\nstd    105058.800004  137618.655600  137577.456205           0.0\nmin         5.000000      11.000000      12.000000           1.0\n25%     90843.000000   39315.000000   39697.000000           1.0\n50%    181718.000000  113964.000000  113489.000000           1.0\n75%    272849.500000  250886.000000  251945.000000           1.0\nmax    363860.000000  493877.000000  493878.000000           1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>135175.000000</td>\n      <td>135175.000000</td>\n      <td>135175.000000</td>\n      <td>135175.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>181735.176741</td>\n      <td>156901.917507</td>\n      <td>157247.986292</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>105058.800004</td>\n      <td>137618.655600</td>\n      <td>137577.456205</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>11.000000</td>\n      <td>12.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>90843.000000</td>\n      <td>39315.000000</td>\n      <td>39697.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>181718.000000</td>\n      <td>113964.000000</td>\n      <td>113489.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>272849.500000</td>\n      <td>250886.000000</td>\n      <td>251945.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>363860.000000</td>\n      <td>493877.000000</td>\n      <td>493878.000000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_copy[train_df_copy['is_duplicate'] > 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90160ef",
   "metadata": {},
   "source": [
    "Total number of unique questions whose duplicates we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7de225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "80105"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_copy[train_df_copy['is_duplicate'] > 0]['qid1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2e678",
   "metadata": {},
   "source": [
    "Total number of unique questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b145935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "266358"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_copy['qid1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20ccab",
   "metadata": {},
   "source": [
    "**Decision** : We could go this path and use triplet loss, however, triplet loss uses A(anchor), P(positive) and N(negative) triplet and it's very important to find a N which is closer to A but still not a duplicate. For us to find those pairs would be a time-taking exercise which I could try to do after basic model, perhaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf077ae",
   "metadata": {},
   "source": [
    "# Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc68527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5af18d",
   "metadata": {},
   "source": [
    "#### Prepare a list of all vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c1a18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "493391"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_set = set(train_df['question1'].unique())\n",
    "q2_set = set(train_df['question2'].unique())\n",
    "all_ques_list = q1_set | q2_set\n",
    "len(all_ques_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74f75f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india? : What is the step by step guide to invest in share market? : 0\n"
     ]
    }
   ],
   "source": [
    "q1_list = train_df['question1'].tolist()\n",
    "q1_list = [str(ques) for ques in q1_list]\n",
    "q2_list = train_df['question2'].tolist()\n",
    "q2_list = [str(ques) for ques in q2_list]\n",
    "is_duplicate_list = train_df['is_duplicate'].tolist()\n",
    "\n",
    "print(q1_list[0],\":\",q2_list[0],\":\",is_duplicate_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28eb9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "931123fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 91014\n"
     ]
    }
   ],
   "source": [
    "all_questions_list = q1_list + q2_list\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(all_questions_list)\n",
    "\n",
    "q1_word_seq = tokenizer.texts_to_sequences(q1_list)\n",
    "q2_word_seq = tokenizer.texts_to_sequences(q2_list)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12820477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c79481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer word index we've gotten for later\n",
    "\n",
    "dictionary = word_index\n",
    "# Let's save this out, so we can use it later\n",
    "with open('..\\\\app\\\\models\\\\dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b12a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a954b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing glove.840B.300d.txt\n",
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import get_file\n",
    "\n",
    "GLOVE_DOWNLOAD_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "\n",
    "if not exists(expanduser('~/.keras/datasets/glove.840B.300d.zip')):\n",
    "    zipfile = ZipFile(get_file('glove.840B.300d.zip', GLOVE_DOWNLOAD_URL))\n",
    "    zipfile.extract('glove.840B.300d.txt', path=expanduser('~/.keras/datasets/'))\n",
    "    \n",
    "print(\"Processing\", 'glove.840B.300d.txt')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(expanduser('~/.keras/datasets/glove.840B.300d.txt'), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c53d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 27054\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.27204001 -0.06203    -0.1884     ...  0.13015001 -0.18317001\n",
      "   0.1323    ]\n",
      " [-0.038548    0.54251999 -0.21843    ...  0.11798     0.24590001\n",
      "   0.22872999]\n",
      " ...\n",
      " [ 0.27021     1.01320004  0.78776002 ...  0.28852999 -0.056837\n",
      "  -0.15815   ]\n",
      " [ 0.73556     0.31016001  0.33723    ...  0.063972   -0.16123\n",
      "   0.59724998]\n",
      " [-0.010027   -0.45328999  0.44459999 ...  0.82262999  0.024273\n",
      "   0.12003   ]]\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "print(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "321f610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[2, 3, 1, 1245, 57, 1245, 2546, 7, 577, 8, 772, 379, 8, 35],\n [2, 3, 1, 562, 10, 13509, 14684, 5, 21440, 4449],\n [4, 13, 5, 219, 1, 439, 10, 17, 364, 1848, 205, 146, 6, 2836],\n [16, 72, 5, 2693, 309, 2764, 4, 13, 5, 661, 19],\n [23, 49, 7202, 8, 233, 33753, 1906, 2077, 10473, 12, 1927, 10671, 6462]]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_word_seq[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827394e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0\n",
    "for ques in q1_word_seq:\n",
    "    if len(ques) > max_seq_length:\n",
    "        max_seq_length = len(ques)\n",
    "\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19b37387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5369fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of question1 data tensor: (363861, 130)\n",
      "Shape of question2 data tensor: (363861, 130)\n",
      "Shape of label tensor: (363861,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 130\n",
    "\n",
    "q1_data = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(is_duplicate_list, dtype=int)\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "202f8724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     2,\n            3,     1,  1245,    57,  1245,  2546,     7,   577,     8,\n          772,   379,     8,    35],\n       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     2,     3,     1,   562,    10, 13509,\n        14684,     5, 21440,  4449],\n       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     4,\n           13,     5,   219,     1,   439,    10,    17,   364,  1848,\n          205,   146,     6,  2836],\n       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,    16,    72,     5,  2693,   309,  2764,     4,\n           13,     5,   661,    19],\n       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n           23,    49,  7202,     8,   233, 33753,  1906,  2077, 10473,\n           12,  1927, 10671,  6462]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0ce38f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(363861, 2, 130)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b8d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "103cd7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(291088, 130)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38a29c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   0,    0,    0, ...,  860, 1940, 2127],\n       [   0,    0,    0, ...,  471, 1170, 2418],\n       [   0,    0,    0, ..., 1526,   40, 9135],\n       ...,\n       [   0,    0,    0, ...,  190,   17,  333],\n       [   0,    0,    0, ...,   68,   46,  270],\n       [   0,    0,    0, ..., 3525,   10,   19]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f363dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(291088, 130)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a276495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dropout, concatenate, Dense, BatchNormalization\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4de019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 130)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 130)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 130, 300)     27304500    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 50)           70200       ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50)           0           ['lstm[0][0]',                   \n",
      "                                                                  'lstm[1][0]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          5100        ['dropout[0][0]',                \n",
      "                                                                  'dropout[1][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 200)          0           ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            201         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,380,001\n",
      "Trainable params: 75,501\n",
      "Non-trainable params: 27,304,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_HIDDEN_UNITS_LAYER1 = 50\n",
    "NUM_HIDDEN_UNITS_LAYER2 = 100\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedding_layer = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)\n",
    "\n",
    "q1 = embedding_layer(question1)\n",
    "q2 = embedding_layer(question2)\n",
    "\n",
    "lstm_first = LSTM(NUM_HIDDEN_UNITS_LAYER1, return_sequences=False)\n",
    "\n",
    "q1 = lstm_first(q1)\n",
    "q2 = lstm_first(q2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "\n",
    "q1 = dropout_layer(q1)\n",
    "q2 = dropout_layer(q2)\n",
    "\n",
    "dense = Dense(100, activation='relu')\n",
    "dropout_two = Dropout(0.2)\n",
    "bn_one = BatchNormalization()\n",
    "\n",
    "q1 = dense(q1)\n",
    "# q1 = dropout_two(q1)\n",
    "# q1 = bn_one(q1)\n",
    "q2 = dense(q2)\n",
    "# q2 = dropout_two(q2)\n",
    "# q2 = bn_one(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97b748ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b645f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"..\\\\app\\\\models\\\\model1.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb2dc297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eb7c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2023-01-25 10:22:45.230628\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 10:22:48.822742: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2023-01-25 10:22:48.965673: I tensorflow/stream_executor/cuda/cuda_blas.cc:1804] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 15s 21ms/step - loss: 0.5564 - accuracy: 0.7163 - val_loss: 0.5251 - val_accuracy: 0.7409\n",
      "Epoch 2/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.5140 - accuracy: 0.7481 - val_loss: 0.5158 - val_accuracy: 0.7441\n",
      "Epoch 3/20\n",
      "569/569 [==============================] - 11s 20ms/step - loss: 0.4989 - accuracy: 0.7568 - val_loss: 0.4984 - val_accuracy: 0.7607\n",
      "Epoch 4/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4867 - accuracy: 0.7662 - val_loss: 0.4947 - val_accuracy: 0.7631\n",
      "Epoch 5/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4768 - accuracy: 0.7717 - val_loss: 0.4878 - val_accuracy: 0.7683\n",
      "Epoch 6/20\n",
      "569/569 [==============================] - 11s 20ms/step - loss: 0.4688 - accuracy: 0.7759 - val_loss: 0.4873 - val_accuracy: 0.7683\n",
      "Epoch 7/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4606 - accuracy: 0.7815 - val_loss: 0.4836 - val_accuracy: 0.7726\n",
      "Epoch 8/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4534 - accuracy: 0.7862 - val_loss: 0.4852 - val_accuracy: 0.7693\n",
      "Epoch 9/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4475 - accuracy: 0.7893 - val_loss: 0.4803 - val_accuracy: 0.7744\n",
      "Epoch 10/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4406 - accuracy: 0.7934 - val_loss: 0.4829 - val_accuracy: 0.7739\n",
      "Epoch 11/20\n",
      "569/569 [==============================] - 11s 20ms/step - loss: 0.4352 - accuracy: 0.7962 - val_loss: 0.4812 - val_accuracy: 0.7745\n",
      "Epoch 12/20\n",
      "569/569 [==============================] - 11s 19ms/step - loss: 0.4294 - accuracy: 0.7996 - val_loss: 0.4801 - val_accuracy: 0.7745\n",
      "Epoch 12: early stopping\n",
      "Training ended at 2023-01-25 10:25:02.132054\n",
      "Minutes elapsed: 2.281686\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "\n",
    "# early stopping\n",
    "es = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# model checkpoint\n",
    "mc = ModelCheckpoint(filepath='..\\\\app\\\\models\\\\question_pairs_weights_type1_final_new.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# callbacks\n",
    "cd = [es, mc]\n",
    "\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=([Q1_test, Q2_test], y_test),\n",
    "                    verbose=1,\n",
    "                    batch_size=512,\n",
    "                    callbacks=cd)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae44ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99493b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text, dictionary):\n",
    "    words = text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "        return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aa97ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.5]]\n"
     ]
    }
   ],
   "source": [
    "# HAPPY CASE\n",
    "question1 = \"What's r programming?\"\n",
    "question2 = \"What's in r programming?\"\n",
    "\n",
    "q1_word_seq = convert_text_to_index_array(question1,dictionary)\n",
    "q1_word_seq = [q1_word_seq]\n",
    "q2_word_seq = convert_text_to_index_array(question2,dictionary)\n",
    "q2_word_seq = [q2_word_seq]\n",
    "q1_data = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred = model.predict([q1_data,q2_data])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "380381d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[[0.48648134]]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"How to learn english?\"\n",
    "question2 = \"Why can't I dance?\"\n",
    "\n",
    "q1_word_seq = convert_text_to_index_array(question1,dictionary)\n",
    "q1_word_seq = [q1_word_seq]\n",
    "q2_word_seq = convert_text_to_index_array(question2,dictionary)\n",
    "q2_word_seq = [q2_word_seq]\n",
    "q1_data = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred = model.predict([q1_data,q2_data])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89acdb",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f60493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66cc950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return keras.backend.exp(-keras.backend.sum(keras.backend.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7ff09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 130)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 130)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 130, 300)     27304500    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 130, 300)     27304500    ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 130, 300)    90300       ['embedding_1[0][0]']            \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 130, 300)    90300       ['embedding_2[0][0]']            \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 300)          0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 300)          0           ['time_distributed_1[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 600)          0           ['lambda[0][0]',                 \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 200)          120200      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 200)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 200)         800         ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 200)          40200       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 200)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 200)         800         ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 200)          40200       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 200)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 200)         800         ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 200)          40200       ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 200)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 200)         800         ['dropout_5[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            201         ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,033,801\n",
      "Trainable params: 423,201\n",
      "Non-trainable params: 54,610,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1,\n",
    "               EMBEDDING_DIM,\n",
    "               weights=[word_embedding_matrix],\n",
    "               input_length=MAX_SEQUENCE_LENGTH,\n",
    "               trainable=False)(question1)\n",
    "q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: keras.backend.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
    "\n",
    "q2 = Embedding(nb_words + 1,\n",
    "               EMBEDDING_DIM,\n",
    "               weights=[word_embedding_matrix],\n",
    "               input_length=MAX_SEQUENCE_LENGTH,\n",
    "               trainable=False)(question2)\n",
    "q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: keras.backend.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"..\\\\app\\\\models\\\\model2.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2023-01-26 18:00:27.471300\n",
      "Epoch 1/25\n",
      " 626/4549 [===>..........................] - ETA: 7:27 - loss: 0.6461 - accuracy: 0.6538"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "\n",
    "# early stopping\n",
    "es = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# model checkpoint\n",
    "mc = ModelCheckpoint(filepath='..\\\\app\\\\models\\\\question_pairs_weights.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# callbacks\n",
    "cd = [es, mc]\n",
    "\n",
    "history_2 = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=([Q1_test, Q2_test], y_test),\n",
    "                    verbose=1,\n",
    "                    batch_size=64,\n",
    "                    callbacks=cd)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
